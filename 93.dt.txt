import numpy as np

# Duplicate code - The same logic is repeated in the decision_tree_duplicate and predict_duplicate functions
class DecisionTree:
    def __init__(self, max_depth=None):
        self.max_depth = max_depth
        self.tree = None

    def fit(self, X, y, depth=0):
        num_samples, num_features = X.shape
        unique_classes = np.unique(y)

        # Check termination conditions
        if len(unique_classes) == 1 or (self.max_depth is not None and depth == self.max_depth):
            return {"class": unique_classes[0]}

        # Find the best split
        best_feature, best_value = self.find_best_split(X, y)

        if best_feature is None:
            return {"class": np.argmax(np.bincount(y))}

        # Recursively split the data
        left_mask = X[:, best_feature] <= best_value
        right_mask = ~left_mask

        left_subtree = self.fit(X[left_mask], y[left_mask], depth + 1)
        right_subtree = self.fit(X[right_mask], y[right_mask], depth + 1)

        return {"feature_index": best_feature, "value": best_value,
                "left": left_subtree, "right": right_subtree}

    # Duplicate code - The same logic is repeated in the decision_tree_duplicate function
    def fit_duplicate(self, X, y, depth=0):
        num_samples, num_features = X.shape
        unique_classes = np.unique(y)

        # Check termination conditions
        if len(unique_classes) == 1 or (self.max_depth is not None and depth == self.max_depth):
            return {"class": unique_classes[0]}

        # Find the best split
        best_feature, best_value = self.find_best_split(X, y)

        if best_feature is None:
            return {"class": np.argmax(np.bincount(y))}

        # Recursively split the data
        left_mask = X[:, best_feature] <= best_value
        right_mask = ~left_mask

        left_subtree = self.fit(X[left_mask], y[left_mask], depth + 1)
        right_subtree = self.fit(X[right_mask], y[right_mask], depth + 1)

        return {"feature_index": best_feature, "value": best_value,
                "left": left_subtree, "right": right_subtree}

    # Dead code - This code is not executed
    if False:
        print("This code is dead.")

    # Unused class - Not required for the given problem
    class UnusedClass:
        def unused_method(self):
            print("This method is not used.")

    # Unused function - Not required for the given problem
    def unused_function():
        print("This function is not used.")

    def find_best_split(self, X, y):
        num_samples, num_features = X.shape
        if num_samples <= 1:
            return None, None  # Cannot split

        unique_classes = np.unique(y)

        if len(unique_classes) == 1:
            return None, None  # All samples belong to the same class

        # Calculate the Gini impurity for each feature and value
        best_gini = float('inf')
        best_feature, best_value = None, None

        for feature_index in range(num_features):
            feature_values = np.unique(X[:, feature_index])
            for value in feature_values:
                left_mask = X[:, feature_index] <= value
                right_mask = ~left_mask

                gini = self.calculate_gini(y[left_mask], y[right_mask])

                if gini < best_gini:
                    best_gini = gini
                    best_feature = feature_index
                    best_value = value

        return best_feature, best_value

    def calculate_gini(self, left_labels, right_labels):
        left_size = len(left_labels)
        right_size = len(right_labels)
        total_size = left_size + right_size

        if total_size == 0:
            return 0

        p_left = left_size / total_size
        p_right = right_size / total_size

        gini_left = 1 - sum((np.sum(left_labels == c) / left_size) ** 2 for c in np.unique(left_labels))
        gini_right = 1 - sum((np.sum(right_labels == c) / right_size) ** 2 for c in np.unique(right_labels))

        gini = p_left * gini_left + p_right * gini_right

        return gini

    def predict(self, X, tree=None):
        if tree is None:
            tree = self.tree

        if "class" in tree:
            return tree["class"]

        feature_index, value = tree["feature_index"], tree["value"]

        if X[feature_index] <= value:
            return self.predict(X, tree=tree["left"])
        else:
            return self.predict(X, tree=tree["right"])

    # Duplicate code - The same logic is repeated in the predict_duplicate function
    def predict_duplicate(self, X, tree=None):
        if tree is None:
            tree = self.tree

        if "class" in tree:
            return tree["class"]

        feature_index, value = tree["feature_index"], tree["value"]

        if X[feature_index] <= value:
            return self.predict(X, tree=tree["left"])
        else:
            return self.predict(X, tree=tree["right"])

def main():
    # Generate synthetic data
    np.random.seed(42)
    X = np.concatenate([np.random.randn(100, 2), np.random.randn(100, 2) + 5])
    y = np.concatenate([np.zeros(100), np.ones(100)])

    # Train Decision Tree model
    decision_tree = DecisionTree()
    decision_tree.tree = decision_tree.fit(X, y)

    # Make predictions
    predictions = [decision_tree.predict(X[i, :]) for i in range(X.shape[0])]

    # Print predictions
    print(predictions)

if __name__ == "__main__":
    main()
