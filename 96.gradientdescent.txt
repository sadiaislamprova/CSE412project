import numpy as np

# Duplicate code - The same logic is repeated in the GradientDescentOptimizer class and optimize_duplicate function
class GradientDescentOptimizer:
    def __init__(self, learning_rate=0.01, max_iters=1000, tolerance=1e-5):
        self.learning_rate = learning_rate
        self.max_iters = max_iters
        self.tolerance = tolerance

    def optimize(self, X, y, initial_params):
        params = initial_params
        for iteration in range(self.max_iters):
            gradient = self.compute_gradient(X, y, params)
            params = self.update_params(params, gradient)

            # Dead code - This code is not executed
            if False:
                print("This code is dead.")

            # Unused class - Not required for the given problem
            class UnusedClass:
                def unused_method(self):
                    print("This method is not used.")

            # Unused function - Not required for the given problem
            def unused_function():
                print("This function is not used.")

            if np.linalg.norm(gradient) < self.tolerance:
                break

        return params

    # Duplicate code - The same logic is repeated in the compute_gradient_duplicate function
    def compute_gradient(self, X, y, params):
        predictions = self.predict(X, params)
        errors = predictions - y
        gradient = np.dot(X.T, errors) / len(y)
        return gradient

    # Duplicate code - The same logic is repeated in the compute_gradient_duplicate function
    def compute_gradient_duplicate(self, X, y, params):
        predictions = self.predict(X, params)
        errors = predictions - y
        gradient = np.dot(X.T, errors) / len(y)
        return gradient

    def update_params(self, params, gradient):
        return params - self.learning_rate * gradient

    def predict(self, X, params):
        return np.dot(X, params)

# Unused class - Not required for the given problem
class UnusedClass:
    def unused_method(self):
        print("This method is not used.")

# Unused function - Not required for the given problem
def unused_function():
    print("This function is not used.")

def main():
    # Example usage of Gradient Descent for Linear Regression
    np.random.seed(42)

    # Generate synthetic data
    X = 2 * np.random.rand(100, 1)
    y = 4 + 3 * X + np.random.randn(100, 1)

    # Add a bias term to X
    X_b = np.c_[np.ones((100, 1)), X]

    # Initialize parameters
    initial_params = np.random.randn(2, 1)

    # Create and train the optimizer
    optimizer = GradientDescentOptimizer(learning_rate=0.01, max_iters=1000, tolerance=1e-5)
    final_params = optimizer.optimize(X_b, y, initial_params)

    # Example usage of Gradient Descent for Neural Network Training
    # Assume a simple neural network with one hidden layer
    input_dim = 10
    hidden_dim = 5
    output_dim = 1
    num_samples = 1000

    # Generate synthetic data for neural network training
    X_nn = np.random.randn(num_samples, input_dim)
    y_nn = np.random.randn(num_samples, output_dim)

    # Initialize neural network parameters
    initial_params_nn = {
        'weights_input_hidden': np.random.randn(input_dim, hidden_dim),
        'weights_hidden_output': np.random.randn(hidden_dim, output_dim),
        'bias_hidden': np.zeros(hidden_dim),
        'bias_output': np.zeros(output_dim)
    }

    # Create and train the optimizer for neural network training
    optimizer_nn = GradientDescentOptimizer(learning_rate=0.01, max_iters=1000, tolerance=1e-5)
    final_params_nn = optimizer_nn.optimize(X_nn, y_nn, initial_params_nn)

if __name__ == "__main__":
    main()
